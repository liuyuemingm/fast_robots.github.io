<!DOCTYPE html>
<html lang="en">
    <head>
        <meta charset="utf-8" />
        <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no" />
        <meta name="description" content="" />
        <meta name="author" content="" />
        <title>Yueming Liu - Fast Robots</title>
        <link rel="icon" type="image/x-icon" href="assets/img/favicon.ico" />
        <!-- Font Awesome icons (free version)-->
        <script src="https://use.fontawesome.com/releases/v6.3.0/js/all.js" crossorigin="anonymous"></script>
        <!-- Google fonts-->
        <link href="https://fonts.googleapis.com/css?family=Saira+Extra+Condensed:500,700" rel="stylesheet" type="text/css" />
        <link href="https://fonts.googleapis.com/css?family=Muli:400,400i,800,800i" rel="stylesheet" type="text/css" />
        <!-- Core theme CSS (includes Bootstrap)-->
        <link href="css/styles.css" rel="stylesheet" />
    </head>
    <body id="page-top">
        <!-- Navigation-->
        <nav class="navbar navbar-expand-lg navbar-dark bg-primary fixed-top" id="sideNav">
            <a class="navbar-brand js-scroll-trigger" href="#page-top">
                <span class="d-block d-lg-none">Yueming Liu</span>
                <span class="d-none d-lg-block"><img class="img-fluid img-profile rounded-circle mx-auto mb-2" src="assets/img/profile.jpg" alt="..." /></span>
            </a>
            <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarResponsive" aria-controls="navbarResponsive" aria-expanded="false" aria-label="Toggle navigation"><span class="navbar-toggler-icon"></span></button>
            <div class="collapse navbar-collapse" id="navbarResponsive">
                <ul class="navbar-nav">
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#About">About</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 1">Lab 1</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 2">Lab 2</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 3">Lab 3</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 4">Lab 4</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 5">Lab 5</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 6">Lab 6</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 7">Lab 7</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 8">Lab 8</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 9">Lab 9</a></li>
                    <li class="nav-item"><a class="nav-link js-scroll-trigger" href="#Lab 10">Lab 10</a></li>

                </ul>
            </div>
        </nav>
        <!-- Page Content-->
        <div class="container-fluid p-0">
            <!-- About-->
            <section class="resume-section" id="About">
                <div class="resume-section-content">
                    <h1 class="mb-0">
                        Yueming
                        <span class="text-primary">Liu</span>
                    </h1>
                    <div class="subheading mb-5">
                        MAE 5190: Fast Robots
                        
                    </div>
                    <p class="lead mb-5">I am an MEng Student in Mechanical Engineering at Cornell University, 
                        please check out my main website <a href="https://yueming-liu.web.app/">here</a>! </p>
                
                </div>
            </section>
            <hr class="m-0" />
            <!-- Lab 1-->
            <section class="resume-section" id="Lab 1">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 1 <span class="text-primary">The Artemis board and Bluetooth</span> </h2>
                    <p class="mb-5">The goal of this lab is to set up the Arduino IDE for the Artemis board, and connect the board to the laptop with Bluetooth and Jupyter Lab notebook.</p>
                        
                    <div class="d-flex flex-column flex-md-row justify-content-between mb-5">
                        <div class="flex-grow-1">
                            <h3 class="mb-3">Part A</h3>

                            <div class="mb-5">
                                <div class="subheading mb-0">Prelab</div>
                                <p>For the prelab, I updated my Arduino IDE to the lastest version and installed the Sparkfun Appollo3 boards manager in the IDE. </p>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 1: Blink</div>
                                <p>To test the basic connectivity of the board, I ran the example script File->Examples->01.Basics </p>
                                <iframe width="420" height="315"
                                    src="https://www.youtube.com/embed/op_ChMm5QYk">
                                </iframe>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 2: Serial Output</div>
                                <p>To test the serial monitor functionality, I set the baud rate of the board and the serial monitor to 115200 and ran the example script File->Examples->Apollo3->Example4_Serial. I was able to see the output when I reset the board. </p>
                                <iframe width="420" height="315"
                                    src="https://www.youtube.com/embed/7u9DyQK-hrk">
                                </iframe>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 3: Analog Read</div>
                                <p>To test temperature sensor on the boad and the analog read functionality, I ran the example script File->Examples->Apollo3->Example2_analogRead. I was able to change the temperature output (the second colomn) by touching the sensor with my fingers. </p>
                                <iframe width="420" height="315"
                                    src="https://www.youtube.com/embed/4Vi-IR_s2ds">
                                </iframe>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 4: Microphone Output</div>
                                <p>To test microphone on the boad, I ran the example script File->Examples->PDM->Example1_MicrophoneOutput. I played an audio clip with an increasing pitch and the board correctly outputted the increasing frequencies. </p>
                                <iframe width="420" height="315"
                                    src="https://www.youtube.com/embed/elPu62MjLh0">
                                </iframe>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 5: Blink at C Note</div>
                                <p>I programmed the board to blink the LED when I play a musical "C" note, and off otherwise. </p>
                                <iframe width="420" height="315"
                                    src="https://www.youtube.com/embed/T_X3ld3etUc">
                                </iframe>
                            </div>




                            <h3 class="mb-3">Part B</h3>

                            <div class="mb-5">
                                <div class="subheading mb-0">Prelab</div>
                                <p>For the prelab, I first installed venv. </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">python3 -m pip install --user virtualenv</code></div></div>
                                <p>I created a Python virtual environement under my project directory and activated it. </p>
                                <div class="code-container"><div class="line-numbers">1<br>2</div>
                                <div class="code-content"><code class="mb-0">python3 -m venv FastRobots_ble<br>source FastRobots_ble/bin/activate</code></div></div>
                                <p>I then installed all the necessary packages. </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">pip install numpy pyyaml colorama nest_asyncio bleak jupyterlab</code></div></div>
                                <p>After that I was able to start the Jupyter server in my project directory. </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">Jupyter lab</code></div></div>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Configurations</div>
                                <p>In connections.yaml, I replaced the artemis_address value with the MAC address printed by my Artemis in the prelab. </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">artemis_address: 'c0:81:85:26:a3:64'</code></div></div>
                                <p>I then generated a new UUID for my Artemis board, </p>
                                <div class="code-container"><div class="line-numbers">1<br>2</div>
                                <div class="code-content"><code class="mb-0">from uuid import uuid4<br>uuid4()</code></div></div>
                                <p>and used this UUID for my BLE_UUID_TEST_SERVICE in ble_arduino.ino and for my ble_service in connections.yaml. </p>
                                
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 1: ECHO</div>
                                <p>For the ECHO command, the Artemis board receives a string message from the computer, augments the string, and sends it back to the computer. </p>
                                <p>On the Artemis board side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15</div>
                                <div class="code-content"><code class="mb-0">case ECHO:
    char char_arr[MAX_MSG_SIZE];

    // Extract the next value from the command string as a character array
    success = robot_cmd.get_next_value(char_arr);
    if (!success)
        return;

    tx_estring_value.clear();
    tx_estring_value.append("Robot says -> ");
    tx_estring_value.append(char_arr);
    tx_estring_value.append(" :)");
    tx_characteristic_string.writeValue(tx_estring_value.c_str());

    break;</code></div></div>
                                <p>On the Computer side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3</div>
                                <div class="code-content"><code class="mb-0">ble.send_command(CMD.ECHO, "hey")<br>s = ble.receive_string(ble.uuid['RX_STRING'])<br>print(s)</code></div></div>
                                <p>Output from the Jupyter notebook: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">Robot says -> hey :)</code></div></div>
                                
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 2: SEND_THREE_FLOATS</div>
                                <p>The SEND_THREE_FLOATS command sends three floats to the Artemis board, and the board extracts and prints the three float values. </p>
                                <p>On the Artemis board side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25</div>
                                <div class="code-content"><code class="mb-0">case SEND_THREE_FLOATS:
    float float1, float2, float3;

    // Extract the next value from the command string as an integer
    success = robot_cmd.get_next_value(float1);
    if (!success)
        return;

    // Extract the next value from the command string as an integer
    success = robot_cmd.get_next_value(float2);
    if (!success)
        return;

    success = robot_cmd.get_next_value(float3);
    if (!success)
        return;

    Serial.print("Three floats: ");
    Serial.print(float1);
    Serial.print(", ");
    Serial.print(float2);
    Serial.print(", ");
    Serial.println(float3);

    break;</code></div></div>
                                <p>On the Computer side: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">ble.send_command(CMD.SEND_THREE_FLOATS, "1.0| 2.0| 3.0")</code></div></div>
                                <p>Output from the Serial Monitor: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">Three floats: 1.00, 2.00, 3.00</code></div></div>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 3: GET_TIME_MILLIS</div>
                                <p>The GET_TIME_MILLIS command makes the robot reply with a string with the value of current time in milliseconds.</p>
                                <p>On the Artemis board side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9</div>
                                <div class="code-content"><code class="mb-0">case GET_TIME_MILLIS:{
    unsigned long timeMillis = millis();
    char buffer[20];
    itoa(timeMillis, buffer, 10);
    tx_estring_value.clear();
    tx_estring_value.append(buffer);
    tx_characteristic_string.writeValue(tx_estring_value.c_str());
}break;</code></div></div>
                                <p>On the Computer side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3</div>
                                <div class="code-content"><code class="mb-0">ble.send_command(CMD.GET_TIME_MILLIS, "")
s = ble.receive_string(ble.uuid['RX_STRING'])
print(s)</code></div></div>
                                <p>Output from the Jupyter notebook: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">43749</code></div></div>
                            
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 4: Notification Handling</div>
                                <p>When we are unsure about when we receive data from the Artemis board, it is important to set up a notification handler instead of directly calling ble.receive_string(ble.uuid['RX_STRING']). </p>
                                <p>Nothing to be done on the Artemis board side. On the Computer side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5</div>
                                <div class="code-content"><code class="mb-0">def notification_handler(uuid, notification):
    s = ble.bytearray_to_string(notification)
    print("Time: " + s)
ble.start_notify(ble.uuid['RX_STRING'], notification_handler)
ble.send_command(CMD.GET_TIME_MILLIS, "")</code></div></div>
                                <p>Output from the Jupyter notebook: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">Time: 43959</code></div></div>
                            </div>



                            <div class="mb-5">
                                <div class="subheading mb-0">Task 5: GET_TIME_LOOP</div>
                                <p>Similar to GET_TIME_MILLIS command, GET_TIME_LOOP gets the current time in milliseconds and sends it to the laptop to be received and processed by the notification handler.</p>
                                <p>On the Artemis board side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13</div>
                                <div class="code-content"><code class="mb-0">case GET_TIME_LOOP:{
    int num_notify = 30;
    for (int i = 0; i < num_notify; i++) {
        unsigned long timeMillis = millis();
        char buffer[20];
        itoa(timeMillis, buffer, 10);
        tx_estring_value.clear();
        tx_estring_value.append(buffer);
        tx_characteristic_string.writeValue(tx_estring_value.c_str());
        Serial.println("Sent current time.");
    }  
}break;</code></div></div>
                                <p>On the Computer side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8</div>
                                <div class="code-content"><code class="mb-0">time = []
def notification_handler_loop(uuid, notification):
    s = ble.bytearray_to_string(notification)
    time.append(s)

ble.start_notify(ble.uuid['RX_STRING'], notification_handler_loop)
ble.send_command(CMD.GET_TIME_LOOP, "")
print(time)</code></div></div>
                                <p>Output from the Jupyter notebook: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">['165447', '165449', '165450', '165452', '165481']</code></div></div>
                                <p>The board sent 5 messages (4 bytes each) in 165481ms - 165447ms = 34ms, so the effective data transfer rate is 5*4/34*1000 = 588 bytes per second. </p>
                            </div>



                            <div class="mb-5">
                                <div class="subheading mb-0">Task 6: SEND_TIME_DATA</div>
                                <p>The SEND_TIME_DATA command is similar to GET_TIME_LOOP, but stores the times in an array before start sending them. </p>
                                <p>On the Artemis board side, first define numTimeStamps and the array timeStamps, and then: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16</div>
                                <div class="code-content"><code class="mb-0">case SEND_TIME_DATA:{
int c = 0;
    while(true){
    timeStamps[c] = (int) millis();
    c++;
        if(c > numTimeStamps-1){
            break;
        }
    }

    for (int i = 0; i < numTimeStamps; i++) {
        tx_estring_value.clear();
        tx_estring_value.append(timeStamps[i]);
        tx_characteristic_string.writeValue(tx_estring_value.c_str());
    }  
}break;</code></div></div>
                                <p>On the Computer side, similar to last step but change the command to: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">ble.send_command(CMD.SEND_TIME_DATA, "")</code></div></div>
                                <p>Output from the Jupyter notebook was an array of size numTimeStamps. </p>
                            </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Task 7: GET_TEMP_READINGS</div>
                                <p>In addition to getting the time stamp, read temperature at each time stamp. </p>
                                <p>On the Artemis board side: </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19</div>
                                <div class="code-content"><code class="mb-0">case GET_TEMP_READINGS:{
int c = 0;
    while(true){
    timeStamps[c] = (int) millis();
    tempReadings[c] = getTempDegF();
    c++;
        if(c > numTimeStamps-1){
            break;
        }
    }
    for (int i = 0; i < numTimeStamps; i++) {
        tx_estring_value.clear();
        tx_estring_value.append("Time: ");
        tx_estring_value.append(timeStamps[i]);
        tx_estring_value.append(", Temp (F): ");
        tx_estring_value.append(tempReadings[i]);
        tx_characteristic_string.writeValue(tx_estring_value.c_str());
    }  
}break;</code></div></div>
                                <p>On the Computer side, similar to last step but change the command to: </p>
                                <div class="code-container"><div class="line-numbers">1</div>
                                <div class="code-content"><code class="mb-0">ble.send_command(CMD.GET_TEMP_READINGS, "")</code></div></div>
                                <p>Output from the Jupyter notebook was an array of size numTimeStamps with the time stamps and the corresponding temperature readings. </p>
                            </div>


                            <div class="mb-5">
                                <div class="subheading mb-0">Task 8: Methods Comparison</div>
                                <p>The first method sends the data live, which makes debuggin quick and easy with the data streaming. 
                                    We are not worried about running out of space on the Artemis board as the data is not being stored there. 
                                    The rate in which it records and sends data, however, is limited because it needs alternate between these two tasks. 
                                    Some data might also get lost in this process. </p>

                                <p>The second method stores all the data on the board and transmits it at some later time. 
                                    It is significantly faster than the first one since it does not need to transmit data while also recording it. 
                                    However, we need to be considerate about the storage space on board. The Artemis board has 384 kB of RAM. 
                                    If each datapoint is 4 bytes, then it runs out of space after 96000 datapoints. </p>
                            </div>


                            <div class="mb-5">
                                <div class="subheading mb-0">Task 9: Effective Data Rate and Overhead </div>
                                <p>We are insterested in the data rate with respect to the size of the packets. 
                                    In order to find this relation, we can command the board to send a series of packets of different sizes to the computer, 
                                    and record the difference between the send time and receive time of each packete. Here the ECHO command returns exactly the message sent:   </p>
                                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16</div>
                                <div class="code-content"><code class="mb-0">packet = []
time_send = []
time_receive = []
data_size = []
def notification_handler_packet(uuid, notification):
    packet.append(ble.bytearray_to_string(notification))

ble.start_notify(ble.uuid['RX_STRING'], notification_handler_packet)
for bytes in range(5, 125, 5):
    data_size.append(bytes)
    packet = []
    time_send.append(time.time())
    ble.send_command(CMD.ECHO, "a"*bytes)
    
    while not packet:
        time_receive.append(time.time())</code></div></div>
                                <p>Output from the Jupyter notebook is shown below. The datarate increases 
                                    linearly with respect to the packet size, indicating that short packets 
                                    introduce significant overhead, whereas big packets helps with amortization. </p>
                                    <img class="mb-2" width="420" src="assets/img/lab1_9.png" alt="..." />
                            </div>


                            <div class="mb-5">
                                <div class="subheading mb-0">Task 10: Reliability </div>
                                <p>After experimenting with sending a large number of messages at a high rate from the Artemis board to the computer, 
                                    the communication appears to be reasonably reliable, and all the data published from the board was received by the computer.  </p>
                                </div>

                            <div class="mb-5">
                                <div class="subheading mb-0">Discussion </div>
                                <p>In this lab I learned about the BLE communication scheme and its restrictions, which is going to be very useful in the future labs when we need to send data from the robot to the computer. 
                                    I was initially struggling with understanding the BLE code structure but was able to make good progress after reading the documentation. 
                                </div>
                            
                        </div>
                    </div>
                </div>
            </section>
            <hr class="m-0" />
            <!-- Lab 2-->
            <section class="resume-section" id="Lab 2">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 2 <span class="text-primary">IMU</span></h2>
                    <p class="mb-5">The goal of this lab is to characterize and test the  Inertial Measurement Unit (IMU). </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Task 1: IMU Setup</div>
                        <p>I installed the necessary library for the ICM 20948 Inertial Measurement Unit (IMU), 
                            connected the IMU to the Artemis board with a QWIIC connector as shown below, and ran the first example in the ICM-20948 Library which prints the IMU measurements.
                            The AD0_VAL is the value of the last bit of the I2C address. Since my ADR jumper is open, the AD0_VAL should be default to 1. </p>
                        <img class="mb-2" width="420" src="assets/img/lab2_1.jpeg" alt="..." />
                        <p>I included start-up blink in the example code as shown in the video. The accelerometer readings do not change significantly when I rotate the sensor around the z-axis slowly; 
                            It changes when I accelerate, flip, or rotate the sensor around a different axis. The Gyroscope readings do not change significantly when I linearly accelerate or slowly rotate the sensor. It changes
                            when I flip or rotate the sensor quickly.  </p>
                        <iframe width="420" height="315"
                            src="https://www.youtube.com/embed/-sq594vBxdk">
                        </iframe>
                    </div>


                    <div class="mb-5">
                        <div class="subheading mb-0">Task 2: Accelerometer</div>
                        <p>I used the equations from class to convert the accelerometer data into pitch and roll. </p>
                        <div class="code-container"><div class="line-numbers">1<br>2</div>
                        <div class="code-content"><code class="mb-0">pitch_a = atan2(myICM.accY(),myICM.accZ())*180/M_PI;<br>roll_a  = atan2(myICM.accX(),myICM.accZ())*180/M_PI;</code></div></div>
                        <p>The picture below shows the output from the Serial Monitor at {-90, 0, 90} degrees pitch and roll. 
                            The accelerometer is fairly accurate, with error bound within +/-0.5 degree. Therefore I do not think a two-point calibration is needed for this sensor: </p>
                        <img class="mb-2" width="840" src="assets/img/lab2_2.JPG" alt="..." />
                        <p>Once the IMU sensors is mounted on the car, it will receive a lot of noise from the environement. In order to filter out some of the noises, we need to first apply Fourier Transform to 
                            the raw data, and from the Fourier Transform we can find a cutoff frequency that separate the noise from the useful information. We can then implement a low pass filter (LPF) with this cutoff frequency to clean up the raw data. 
                            The following plots show the raw data from some test runs and the Fourier Transforms of the pitch and roll readings from the accelerometer: 
                        </p>
                        <img class="mb-2" height="315" src="assets/img/lab2_3.png" alt="..." />
                        <img class="mb-2" height="315" src="assets/img/lab2_4.png" alt="..." />
                        <img class="mb-2" height="315" src="assets/img/lab2_5.png" alt="..." />
                        <img class="mb-2" height="315" src="assets/img/lab2_6.png" alt="..." />
                        <p>As shown in the FFT plots above, there is a peak of noise at around 50Hz for both pitch and roll data, 
                            and 10Hz appears to be a good cut-off frequency in this case, although this number is subject 
                            to change after the sensor is mounted on the robot. The implementation of the low pass filter and the results are shown below. 
                        After applying the filter, the data preserves the overall trend but is much less noisy:</p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9</div>
                        <div class="code-content"><code class="mb-0">fc = 10
T = (time_a[-1] - time_a[0])/(len(time_a)-1)
RC = 1 / (2 * np.pi * fc)
alpha = T / (T + RC)
pitch_a_LPF = [0]*len(pitch_a)

for n in range(1, len(pitch_a)):
    pitch_a_LPF[n] = alpha*pitch_a[n] + (1-alpha)*pitch_a_LPF[n-1]
    pitch_a_LPF[n-1] = pitch_a_LPF[n]</code></div></div>
                        <img class="mb-2" height="315" src="assets/img/lab2_7.png" alt="..." />
                        <img class="mb-2" height="315" src="assets/img/lab2_8.png" alt="..." />
                        </div>
                        



                    <div class="mb-5">
                        <div class="subheading mb-0">Task 3: Gyroscope</div>
                        <p>I used the equations from class to convert the gyroscope data into pitch, roll, and yaw by estimating the angles using angle derivatives from the myICM.gyrX(), myICM.gyrY(), and myICM.gyrZ() output: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3</div>
                        <div class="code-content"><code class="mb-0">pitch_g = pitch_g + myICM.gyrX()*dt;<br>roll_g = roll_g + myICM.gyrY()*dt;<br>yaw_g = yaw_g + myICM.gyrZ()*dt;</code></div></div>
                        <p>As shown below, for both the pitch and roll data, the overall trend from the accelerometer 
                            and the gyroscope match closely. However, data from gyroscope appears to have a increasing 
                            drift, despite being more accurate at the beginning and less noisy overall. In order to 
                            account for the drift in the gyroscope and the noise and inaccuracy in the accelerometer, 
                            I applied a complementary filter with alpha = 0.01: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2</div>
                        <div class="code-content"><code class="mb-0">pitch = (pitch+myICM.gyrX()*dt)*(1-alpha_cf) + pitch_a*alpha_cf;<br>roll = (roll+myICM.gyrY()*dt)*(1-alpha_cf) + roll_a*alpha_cf;</code></div></div>
                        <img class="mb-2" width="420" src="assets/img/lab2_9.png" alt="..." />
                        <img class="mb-2" width="420" src="assets/img/lab2_10.png" alt="..." />
                        <img class="mb-2" width="420" src="assets/img/lab2_11.png" alt="..." />
                        <p>For the plot below, I lowered the sampling frequency, and the angle estimation appears to be a lot worse eventhough the overall trend still match up between the accelerometer and the gyroscope. </p>
                        <img class="mb-2" width="420" src="assets/img/lab2_12.png" alt="..." />
                        
                        
                        
                            
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">Task 4: Sample Data</div>
                        <p>I sped up the execution of my loop for the Artemis board by removing all the delays 
                            and Serial print statements. </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25</div>
                        <div class="code-content"><code class="mb-0">case GET_IMU_ACC_READINGS:{
int c = 0;
    while(true){
        if (myICM.dataReady())
        {
            timeStamps[c] = (int) millis();
            myICM.getAGMT(); 
            pitch_a_data[c] = get_pitch_a();
            roll_a_data[c] = get_roll_a();
        }
        c++;
        if(c > numTimeStamps-1){
            break;
        }
    }
    for (int i = 0; i < numTimeStamps; i++) {
        tx_estring_value.clear();
        tx_estring_value.append(timeStamps[i]);
        tx_estring_value.append("|");
        tx_estring_value.append(pitch_a_data[i]);
        tx_estring_value.append("|");
        tx_estring_value.append(roll_a_data[i]);
        tx_characteristic_string.writeValue(tx_estring_value.c_str());
    }  
}break;</code></div></div>
                        <p>As shown below, the sampled values are stored in the form of
                            'time_stamp | pitch_a | roll_a | pitch_g_raw | roll_g_raw | yaw_g_raw', 
                               and I was able to collect 1000 data points in 5.567s, so the sampling rate is 179 data points/second. 
                               The main loop on my Artemis board does not run faster than the IMU produces new values.</p>
                        <img class="mb-2" width="840" src="assets/img/lab2_13.png" alt="..." />

                        <p>It worked for me to have separate arrays for storing accelerometer and gyroscope data as sometimes we 
                            need to access them separately for calculations. However, I did combine them together with the timestamp 
                            into one string for data transmission. This allows me to send over information of different data types 
                            (int and float) at once, reducing the overhead for data transmission. </p>
                        <p>The Artemis board has 384 kB of RAM, and each datapoint (1x int and 5x float) is 24 Bytes, 
                            meaning that it can take at most 16,000 data points. With a rate of 179 data points/second, 
                            this memory corresponds to 89 seconds of IMU data.</p>
                        <p>All plots in Task 3. Gyroscope (the second above) demonstrates that my board can capture at least 5s worth of IMU data 
                            and send it over Bluetooth to the computer.</p>
                    </div>


                    <div class="mb-5">
                        <div class="subheading mb-0">Task 5: Record a Stunt</div>
                        <p>Here is the car doing a backflip! The easiest way to do a backflip is to reverse direction rapidly or have the robot bump into an obstacle. </p>
                        
                            <iframe width="420" height="315"
                            src="https://www.youtube.com/embed/Ibk4pZxtz9s">
                        </iframe>
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">Discussion</div>
                        <p>It is important to use the correct filter for different data, and be mindful about how the data is stored and transmitted as we incorporate more sensors. </p>
                        
                    </div>

                    
                </div>
            </section>
            <hr class="m-0" />
            <!-- Lab 3-->
            <section class="resume-section" id="Lab 3">
                <div class="resume-section-content">

                    
                    <h2 class="mb-0">Lab 3 <span class="text-primary">ToF Sensor</span></h2>
                    <p class="mb-5">The goal of this lab is to incorporate and characterize two Time-of-Flight (ToF) sensors in parallel. </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Prelab</div>
                        <p>According to the <a href="https://cdn.sparkfun.com/assets/8/9/9/a/6/VL53L0X_DS.pdf">datasheet</a>, the ToF sensor uses the I2C protocal with the default address 0x52. Therefore, in order to use two ToF sensors at the same time, we need to change the address of one of the sensors. We can do that by connecting the XSHUT pin of one ToF sensor to a GPIO pin, and after each reboot: </p>
                        <p>1. Turn off sensor 1 by setting XSHUT low. <br>
                           2. Now that we are able to uniquely address sensor 2, change its address.<br>
                           3. Turn on sensor 1 by setting XSHUT high. </p>
                        <p>Thus we are able to address two sensors simutaneously. A wiring diagram is shown below, note that here the XSHUT of sensor 1 is connected to GPIO 8. </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_1.jpeg" alt="..." />
                        <p>In terms of sensor placements, I'm planning to put one sensor at the front of the car to detect obstacles in its path, and another on the side to facilitate tasks like wall-following or map-reconstruction. 
                            With this setup, I won't be able to detect obstacles on the other side or at the back. </p>
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">Connections</div>
                        <p>I soldered the JST connectors to the battery, and the QWIIC connectors to the ToF sensors. 
                            The picture below shows the connections between ToF sensors, IMU, Artemis Nano, and the QWIIC breakout board. </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_2.jpeg" alt="..." />
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">I2C Scanning</div>
                        <p>I ran the example File->Examples->Apollo3->Example05_Wire_I2C to identify the address of the ToF sensor. 
                            The displayed address is 0x29 (or 0b00101001 in binary) as shown below, which does not match the 0x52 (or 0b01010010 in binary) on the datasheet. 
                            This is because the ToF uses the last bit to specify the data direction (0 for read, 1 for write), and the example code only reads the first 7 bits.
                            We can verify this by checking that 0x29 is indeed 0x52 left-shifted by 1 bit. 
                        </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_3.png" alt="..." />
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">ToF Sensor Modes</div>
                        <p>
                            The VL53L1X ToF sensor offers three distance modes: Short, Medium, and Long. 
                            Short mode (max 1.3 m) is more resistant to ambient light, making it ideal for close-range obstacle detection. 
                            Long mode (max 4 m) provides the farthest range but is significantly affected by ambient light. 
                            Medium mode (max 3 m) offers a balance between range and light resistance. <br>
                            I tested the sensors measurements by setting them at certain distances away from a wall and take measurements. A setup of the test is shown below. 
                        </p>

                        <img class="mb-2" width="420" src="assets/img/lab3_4.jpeg" alt="..." />
                        <p>I chose the short mode since 1.3m appears to be a reasonable maximum distance for the robot to detect obstacles. 
                            As shown below, both sensors are able to detect distances up to about 1.7m, and with decent accuracy. 
                            I took 50 measurements for each distance for each sensor, and the variances are shown as error bars below. 
                            The fact that the variances are small and that both sensor data closely align show that the sensor readings are repeatable. 
                        </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_5.png" alt="..." />
                        <p>The ranging time varies between 55ms for distance of ~54mm to 35ms for distance of ~1580mm, calculated from the screenshots below.  </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_6.png" alt="..." />
                        <img class="mb-2" width="420" src="assets/img/lab3_7.png" alt="..." />
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">ToF Sensor Parallel Connection</div>
                        <p>As described in the prelab, in order for the sensor to run in parallel, one of them needs to have the address reconfigured 
                            after each reboot:  </p>

                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6</div>
                        <div class="code-content"><code class="mb-0">pinMode(8, OUTPUT); 
digitalWrite(8, LOW); // shut down sensor 1
distanceSensor2.setI2CAddress(addr); // set address of sensor 2
... // confirm sensor 2 is online
digitalWrite(8, HIGH); // turn on sensor 1
... // confirm sensor 1 is online</code></div></div>
                        <p>Both sensors are outputting at the same time as expected. Notice that sensor 2 consistently outputs a higher value, therefore an offset needs to be added in the future:  </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_8.png" alt="..." /><br>
                        <iframe width="420" height="315"
                                    src="https://www.youtube.com/embed/4q0z7uQQPBY">
                                </iframe>
                        
                    </div>


                    <div class="mb-5">
                        <div class="subheading mb-0">ToF Sensor Speed</div>
                        <p>I removed the delay in the code and only output the sensor readings when the data is ready. 
                            The delay between sensor readings are around 40ms, which is much slower than the Artemis clock speed, 
                            therefore the limiting factor is the sensor reading speed. </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14</div>
                        <div class="code-content"><code class="mb-0">if (distanceSensor1.checkForDataReady()){
int distance1 = distanceSensor1.getDistance();
distanceSensor1.clearInterrupt();
distanceSensor1.stopRanging();
Serial.print("Distance 1(mm): ");
Serial.println(distance1);
}
if (distanceSensor2.checkForDataReady()){
int distance2 = distanceSensor2.getDistance();
distanceSensor2.clearInterrupt();
distanceSensor2.stopRanging();
Serial.print("Distance 2(mm): ");
Serial.println(distance2);
}</code></div></div>
                    </div>


                    <div class="mb-5">
                        <div class="subheading mb-0">ToF + IMU</div>
                        <p>The following show the ToF sensor and IMU data collected at the same time by waving hand in front of the ToF sensor and swining the IMU sensor. Unfortunately my second ToF broke at this point so I was only able to record data from one ToF sensor. </p>
                        <img class="mb-2" width="420" src="assets/img/lab3_9.png" alt="..." />
                        
                    </div>


                    <div class="mb-5">
                        <div class="subheading mb-0">Discussion</div>
                        <p>Infrared sensors include Active IR sensors and Passive IR sensors. 
                            Active IR sensors are low-cost, simple, and fast but have limited range, 
                            making them ideal for short-range applications. 
                            The ToF sensors are an example of Active IR sensors. 
                            Passive IR sensors are energy-efficient, but are more sensitive to environmental changes. <br>

                            I tested the sensors on the white walls, white blankets, and dark-blue blankets with the same distances to see if textures and colors affect the readings. 
                            I noticed that sensors readings are similar and accurate for the wall and the white blankets, but slightly smaller for the dark-blue blankets. 
                            From this prelimiary experient it appears that the sensors are more sensitive to colors than textures. 
                        </p>
                        
                    </div>
                    
                </div>
            </section>
            <!-- Lab 4-->
            <section class="resume-section" id="Lab 4">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 4 <span class="text-primary">Motors and Open Loop Control</span></h2>
                    <p class="mb-5">The goal of Lab 4 is to fully put together the robot and be able to drive it with open loop control. </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Prelab</div>
                        <p>The following is a diagram of the connections between the Artemis board, the motor drivers, and the motors. 
                            Here pins 4, 5, 11, 12 are used on the Artemis board to driver the motors. 
                            In order to supply the motors with a larger current than the upper limit of the motor driver pins, 
                            we parallel-couple the two inputs and outputs on each dual motor driver. Additionally, we use separate 
                            batteries that share the same ground for the board and the motors to decouple their performance and reduce noise.</p>
                        <img class="mb-2" width="420" src="assets/img/lab4_1.jpg" alt="..." />
                    </div>


                    <div class="mb-5">
                        <div class="subheading mb-0">Motor driver tests</div>
                        <p>
                            Before connecting all components to the robot, I first tested the motor drivers using the power supply to ensure 
                            the output PWM signals are correct. Below is the setup of one of the motor drivers hooked up to the oscilloscope
                            and the power supply. The voltage setting in the power supply is 3.7 V, which matches the output of the batteries. 
                        </p>
                        <img class="mb-2" width="420" src="assets/img/lab4_2.jpg" alt="..." />
                        <p>
                            The code used to drive the motor drivers: 
                        </p>

                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10</div>
                        <div class="code-content"><code class="mb-0">#define AB1IN_R 12 // green
#define AB2IN_R 11 // yellow
void setup() {
    pinMode(AB1IN_R,OUTPUT);
    pinMode(AB2IN_R,OUTPUT);
}
void loop() {
    analogWrite(AB1IN_R,100); 
    analogWrite(AB2IN_R,0);
}</code></div></div>
                        <p>Outputs from the oscilloscope change as expected when the analogWrite values change. 
                            Between the first and second image below, I flipped the signal of analogWrite and thus changed 
                            the output for the green wire from High to Low. A smaller wave signal is reflected in the oscilloscope: </p>
                        <img class="mb-2" width="420" src="assets/img/lab4_3.jpeg" alt="..." />
                        <img class="mb-2" width="420" src="assets/img/lab4_4.jpeg" alt="..." />
                    </div>

                    <div class="mb-5">
                        <div class="subheading mb-0">Spinning Wheels</div>
                        <p>After ensuring both motor drivers work, I proceeded to wire all components to the car
                            and was able to spin both the left and right wheels with the power supply.</p>
                        <iframe width="420" height="315"
                            src="https://www.youtube.com/embed/10wRcF9CLVg">
                        </iframe>
                        <p>The code used in this section is similar to the last one: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10</div>
                        <div class="code-content"><code class="mb-0">#define AB1IN_R 12 // green
#define AB2IN_R 11 // yellow
void setup() {
    pinMode(AB1IN_R,OUTPUT);
    pinMode(AB2IN_R,OUTPUT);
}
void loop() {
    analogWrite(AB1IN_R,250); 
    analogWrite(AB2IN_R,0);
}</code></div></div>
                    </div>
                        <p>After both wheels were able to spin as expected, I connected the motor drivers to the batteries.
                            The following video show all wheels running on the battery. 
                        </p>
                        <iframe width="420" height="315"
                            src="https://www.youtube.com/embed/EfQTkmApGKQ">
                        </iframe>
                        <p>Below are pictures of all the components secured in the car: </p>
                        <img class="mb-2" width="630" src="assets/img/lab4_5.jpg" alt="..." />
                    
               

                <div class="mb-5">
                    <div class="subheading mb-0">PWM and Calibartion</div>
                    <p>The range of PWM signal is from 0 to 255. In order to find the lower limits of PWM that allow the robot to move,
                        I gradually increment the value passed into analogWrite from 0 to up to 65. It appears that the left wheel 
                        constantly encounters more resistance than the right wheel, and a offset of 20 is needed for them to achieve 
                        the same performance. Therefore the lower limits of PWM are 65 for the left wheel and 45 for the right wheel. 
                        After applying this offset the robot is able to move in a straight line. Below is a video showing that the robot
                        stays within the center lane for about 7ft. </p> 
                        <iframe height="315"
                            src="https://www.youtube.com/embed/NWMHxK7wReU">
                        </iframe>
                    
                </div> 

                <div class="mb-5">
                    <div class="subheading mb-0">Open Loop Control</div>
                    <p>Below is an open loop control for the robot to stop in front of the door with a slight turn.  </p> 
                        <iframe height="315"
                            src="https://www.youtube.com/embed/zy4RMzVPdkM">
                        </iframe>

                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8</div>
                        <div class="code-content"><code class="mb-0">void loop() {
    forward(200);
    delay(3000);
    turn_right(200);
    delay(300);
    stop();
    delay(10000);
}</code></div></div>

                        <p>Functions used are defined as follows, for example: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6</div>
                        <div class="code-content"><code class="mb-0">void forward(int dc){
    analogWrite(AB1IN_R,0); 
    analogWrite(AB2IN_R,dc);
    analogWrite(AB1IN_L,0); 
    analogWrite(AB2IN_L,dc+20); 
}</code></div></div>
                    
                </div> 


                <div class="mb-5">
                    <div class="subheading mb-0">PWM Limit to Stop Motion</div>
                    <p>Once the robot is in motion, it requires a smaller PWM value to continue moving foward. 
                        I used the following code to determine this lower PWM limit, which decreases the PWM value 
                        every few seconds until we observe that the robot stops. The lower limit is about 25 for the right wheel
                        and 45 for the left wheel. My robot has sticky wheels and does not have consistent PWM that keeps it moving at the lowest speed. </p> 
                    <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7</div>
                    <div class="code-content"><code class="mb-0">int PWM = 50;
void loop() {
    forward(PWM);
    delay(3000);
    blink();
    PWM = PWM - 5;
}</code></div></div>
                    
                </div> 


                <div class="mb-5">
                    <div class="subheading mb-0">Speed of analogWrite()</div>
                    <p>The following code is used to check the analog write frequency: </p> 
                    <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6</div>
                    <div class="code-content"><code class="mb-0">float pre_t = 0;
void loop() {
    analogWrite(AB1IN_R,100); 
    Serial.println(micros()-pre_t);
    pre_t = micros();
}</code></div></div>
                    <img class="mb-2" width="210" src="assets/img/lab4_6.png" alt="..." />
                    <p>It appears that the delay between analog writes is around 5.1ms. 
                        This is comparable to my IMU delay found in Lab 2 (~5.5ms). 
                        Since a general rule of thumb of feedback control is that the sampling frequency (IMU)
                        should be significantly higher than the control frequency (analog write), this delay time 
                        for analog write is acceptable. Manually configuring the timers to generate a faster PWM signal
                        could potentially create a smoother output for quick response in the robot, but will not 
                    have significant impact on the feedback control performance. </p>
                    
                </div> 

            </div>
            </section>
            <!-- Lab 5-->
            <section class="resume-section" id="Lab 5">
                <div class="resume-section-content">
                    
                
                    <h2 class="mb-0">Lab 5 <span class="text-primary">Linear PID control</span></h2>
                    <p class="mb-5">The goal of this lab is to drive the robot towards a wall and stop it 1ft in front of the wall. </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Prelab</div>
                        <p> In order to make debugging and tuning the controller easier during the lab, I set up the program such that the robot  
                            start on an input from my computer sent over Bluetooth, where the numbers passed in are Kp, Ki, Kd, and maximum speed percentage: </p>
                            <div class="code-container"><div class="line-numbers">1<br>2</div>
                    <div class="code-content"><code class="mb-0">ble.start_notify(ble.uuid['RX_STRING'], pid_position_control)
ble.send_command(CMD.PID_POSITION, "0.03|0.001|1|50")</code></div></div>
                        <p>Upon receiving the Bluetooth command, the robot will run the PID controller for a fixed amount of time: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14</div>
                        <div class="code-content"><code class="mb-0">case PID_POSITION:
    Extract the next values from the command string and save them as Kp, Ki, Kd, max_speed_percentage
    initialize ToF sensor
    initilize motor

    while time is less than PID running time 
        record current time stamp in an array
        run get_ToF() and record current ToF data in an array
        run position_PID() to set the controller speed
        drive the robot with controller speed
        record Kp, Ki, and Kd terms in separate arrays
    
    set robot speed to zero
    send over recorded data to computer via Bluetooth
</code></div></div>
                </div>


                <div class="mb-5">
                    <div class="subheading mb-0">Tuning Controller</div>
                    <p>I rescaled the PWM to speed percentage which ranges from 0 to 100.</p>
                    <p>First I implemented the P controller with Kp = 0.01. This value is chosen such 
                        that there it will not be able to overcome static disturbance when it's close enough to 1ft from the wall,
                        as shown below in the plot and the video.  
                        Later I'll demonstrate that adding I controller will make up for that. 
                    </p>
                    <img class="mb-2" width="420" src="assets/img/lab5_1.png" alt="..." /><br>
                    <iframe width="420" height="315"
                    src="https://www.youtube.com/embed/nvVrYGqPiDs">
                </iframe>

                    <p>I then added the I controller with partial wind-up protection with Kp = 0.015 (a small increase from
                        before but still not able to overcome static friction) and Ki = 0.0001. With this additional control, 
                        robot is able to overshoot slightly and back up properly. </p>
                    <img class="mb-2" width="420" src="assets/img/lab5_2.png" alt="..." /><br>
                    <iframe width="420" height="315"
                    src="https://www.youtube.com/embed/NiE1tCG5g6c">
                </iframe>

                    <p>I then added the D controller to provid some damping and reduce the overshoot. 
                        The plot below is with Kp = 0.015, Ki = 0.0002, and Kd = 1. Notice that the Kd term counter acts the 
                        other terms to provide some damping. I decided that the derivative kick prevention is not necessary here
                        because we do not need to change the set point during the run. 
                    </p>
                    <img class="mb-2" width="420" src="assets/img/lab5_3.png" alt="..." />

                    <p>The complete code for the controller is below: </p>
                    <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23</div>
                        <div class="code-content"><code class="mb-0">void position_PID (float meas_dist, int current_time, int prev_time, float prev_err) {
    int dt = current_time - prev_time; 
    float err = meas_dist - set_dist;
    float derivative_err = (err - prev_err) / dt;
    float PID = Kp * err + Ki * (integral_err + (err * dt)) + Kd * derivative_err; 
    
    // anti wind-up
    bool clipping = (abs(PID)>max_speed_percent); 
    bool control_sign_match = (err * PID >= 0); 
    
    if (abs(err)<=10) { 
        integral_err = 0; 
    }else if (!clipping && !control_sign_match){ 
        integral_err = integral_err + (err * dt);
    } 
    
    Kp_term = Kp * err;
    Ki_term = Ki * integral_err;
    Kd_term = Kd * derivative_err;
    speed_percent = Kp_term + Ki_term + Kd_term;
    speed_percent_2_PWM();
    drive(PWM_R, PWM_L);
}
</code></div></div>
                

                    <p>With PID fully implemented, I was able to tune them to a faster speed without running into wall. 
                        The highest velocity calculated from the plot below is 1.6m/s, with Kp = 0.05, Ki = 0.0001, Kd = 1, and 70% of highest speed. </p>
                        <img class="mb-2" width="420" src="assets/img/lab5_4.png" alt="..." />
                        <br>
                        <iframe width="420" height="315"
                        src="https://www.youtube.com/embed/-eKCg7-ynjE">
                    </iframe>

                </div>

                <div class="mb-5">
                    <div class="subheading mb-0">Range/Sampling Time Discussion</div>
                    <p>I was able to decouple the ToF sampling rate from the controller output rate 
                        by using the latest data available without waiting for the ToF sensor to sample.
                        To achieve this, I removed the delay in the function get_ToF() in line 8 in the pseudo code section in Prelab. 
                        The sampling delay for ToF found in lab 3 was about 55ms, and the control delay here, 
                        found from (final time - start time)/number of samples, is about 9ms. </p>
                    <p>I used the long mode for ToF sensor to meet the requirements of starting from 2-4m from the wall. 
                    </p>
                </div>

                <div class="mb-5">
                    <div class="subheading mb-0">Wind-up Discussion</div>
                    <p>Lines 8 - 9 and 13 in the code from the previous section is implemented for wind-up protection. 
                        Line 8 pauses integration when the controller is saturated, which is important because we loose control when the 
                        motor is saturated. This is especially useful when Kp gain is high.  
                        Line 9 pauses integration when the control direction matches the error direction, which is important especially for 
                        low Kp gains because it can significantly reduce the overshoot. Below is a demonstration without wind-up: </p>
                        <iframe width="420" height="315"
                        src="https://www.youtube.com/embed/Wryk5VMNiQM">
                    </iframe>
                        <p>With wind-up, the system behaves well even with different floor conditions. </p>
                        <iframe width="420" height="315"
                        src="https://www.youtube.com/embed/JBXl523rkIE">
                    </iframe>

                </div>

                




            </section>
            <!-- Lab 6-->
            <section class="resume-section" id="Lab 6">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 6: Rotation PID control</h2>
                    <p class="mb-5">The goal of this lab is to use PID control on IMU sensor to maintain robot orientation.  </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Prelab</div>
                        <p> Similar to lab 5, in order to simplify debugging and tuning the controller, I set up the program such that the robot  
                            start on an input from my computer sent over Bluetooth, where the numbers passed in are Kp, Ki, Kd, and maximum speed percentage: </p>
                            <div class="code-container"><div class="line-numbers">1<br>2</div>
                    <div class="code-content"><code class="mb-0">ble.start_notify(ble.uuid['RX_STRING'], pid_rotation_control)
ble.send_command(CMD.PID_ROTATION, "0.0|0.0|0.0|50")</code></div></div>
                        <p>Upon receiving the Bluetooth command, the robot will run the PID controller for a fixed amount of time: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14</div>
                        <div class="code-content"><code class="mb-0">case PID_ROTATION:
    Extract the values from the command string and save them as Kp, Ki, Kd, max_speed_percentage
    initialize IMU sensor
    initilize motor

    while time is less than PID running time 
        record current time stamp in an array
        run get_IMU() and record current IMU data in an array
        run rotation_PID() to set the controller speed
        drive the robot with controller speed
        record Kp, Ki, and Kd terms in separate arrays
    
    set robot speed to zero
    send over recorded data to computer via Bluetooth
</code></div></div>
                </div>

                <div class="mb-5">
                    <div class="subheading mb-0">Tuning Controller</div>
                    <p>I rescaled the PWM to rotational speed percentage which ranges from 0 to 100.</p>
                    <p>First I implemented the P controller with Kp = 1. This value is chosen to be small enough such 
                        that the robot will not be able to overcome small disturbances. 
                        As shown below in the plot and the video, the robot is stuck in the disturbed position. 
                        Later I'll demonstrate that adding I controller will make up for that. 
                    </p>
                    <img class="mb-2" width="420" src="assets/img/lab6_1.png" alt="..." /><br>
                    <iframe width="420" height="315"
                    src="https://www.youtube.com/embed/xDWlQeyxO0w">
                </iframe>

                    <p>I then added the I controller with partial wind-up protection with Kp = 1 and Ki = 3. 
                        With this additional control, robot is able to slowly adust itself back to the original position. </p>
                    <iframe width="420" height="315"
                    src="https://www.youtube.com/embed/KUroMBr6Zss">
                </iframe>


                    <p>The complete code for the controller is below: </p>
                    <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23</div>
                        <div class="code-content"><code class="mb-0">void rotation_PID (float meas_rot, int current_time, int prev_time, float prev_err) {
    int dt = current_time - prev_time; 
    float err = meas_rot - set_rot;
    float derivative_err = (err - prev_err) / dt;
    float PID = Kp * err + Ki * (integral_err + (err * dt)) + Kd * derivative_err; 
    
    // anti wind-up
    bool clipping = (abs(PID)>max_rot_speed_percent); 
    bool control_sign_match = (err * PID >= 0); 
    
    if (abs(err)<=10) { 
        integral_err = 0; 
    }else if (!clipping && !control_sign_match){ 
        integral_err = integral_err + (err * dt);
    } 
    
    Kp_term = Kp * err;
    Ki_term = Ki * integral_err;
    Kd_term = Kd * derivative_err;
    rot_speed_percent = Kp_term + Ki_term + Kd_term;
    rot_speed_percent_2_PWM();
    drive(PWM_R, PWM_L);
}
</code></div></div>
                

                    <p>With PID fully implemented, the robot is able to behave as expected with Kp = 4, Ki = 0.5:  </p>
                        
                        <iframe width="420" height="315"
                        src="https://www.youtube.com/embed/wSMlGVK4IcI">
                    </iframe>

                </div>

                <div class="mb-5">
                    <div class="subheading mb-0">Sampling Time Discussion</div>
                    <p>I was able to decouple the IMU sampling rate from the controller output rate 
                        by using the latest data available without waiting for the IMU sensor tgito respond.
                        The sampling delay for ToF found in previous labs was about 18ms, and the control delay is about 9ms. </p>
                    
                </div>

                <div class="mb-5">
                    <div class="subheading mb-0">Wind-up Discussion</div>
                    <p>Lines 8 - 9 and 13 in the code from the previous section is implemented for wind-up protection. 
                        Line 8 pauses integration when the controller is saturated, which is important because we loose control when the 
                        motor is saturated. This is especially useful when Kp gain is high.  
                        Line 9 pauses integration when the control direction matches the error direction, which is important especially for 
                        low Kp gains because it can significantly reduce the overshoot </p>
                        
                </div>
            </section>         
            <!-- Lab 7-->
            <section class="resume-section" id="Lab 7">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 7 <span class="text-primary">Karman Filter</span></h2>
                    <p class="mb-5">The goal of this lab is to implement Karman Filter on the ToF sensor data to allow for faster execution of the controller. </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Task 1. Estimate Drag and Momentum</div>
                        <p>In order to implement the Karman Filter, we first need to model the system with state-space representation.
                            The following equations describe the steps of obtaining the dynamic equation, where x is the distance between the 
                            car and the wall, u is the control input, and d is the coefficient of drag force. 
                        </p>
                        <img class="mb-2" width="140" src="assets/img/lab7_1.png" alt="..." />
                        <img class="mb-2" width="210" src="assets/img/lab7_3.png" alt="..." /><br>
                        <p>The unknown parameters are d and m, which we can obtain from an open loop run of the robot with a step input. 
                            <br>
                            When the velocity reaches steady state, the acceleration is zero, and we can calculate the drag coefficient:
                        </p>
                        <img class="mb-2" width="230" src="assets/img/lab7_2.png" alt="..." /><br>
                        <p>
                            This is a first-order system for velocity. As the system is reaching the steady state speed, 
                            we can express the velocity in terms of time as shown in the following equations.
                            By finding the 90% rise time and 90% velocity, we can calculate the equivalent mass of the system. 
                        </p>
                        <img class="mb-2" width="360" src="assets/img/lab7_4.png" alt="..." /><br>
                        <p>Finally, using the parameters found, we can construct the state space representation of the car: </p>
                        <img class="mb-2" width="300" src="assets/img/lab7_5.png" alt="..." /><br>
                        <p>Running the car at about 55% of the maximum motor speed while collecting ToF gives the following data: </p>
                        <img class="mb-2" width="420" src="assets/img/lab7_6.png" alt="..." /><br>
                        <p>With simple processing of the data, I found the steady state speed to be about 2.43mm/ms, 
                            and the time to reach 90% of steady state speed is about 925ms: </p>
                        <img class="mb-2" width="420" src="assets/img/lab7_7.png" alt="..." /><br>
                        <p>From there, we can estimate the drag coefficient d = 55/2.43 = 22.6, and equivalent mass m = -22.6*925/ln(0.1) = 9090. </p>


                     </div>

                     <div class="mb-5">
                        <div class="subheading mb-0">Task 2. Initialize KF</div>
                        <p>We have the values for A and B matrices from the previous section. 
                            Since both the ToF sensor collects data and the controller sends commands at discontinuous time steps,
                            it is necessary to discretize our state space representation: </p>
                            <img class="mb-2" width="360" src="assets/img/lab7_8.png" alt="..." /><br>
                        <p>Here dt is the time between each loop of the control signal. 
                            From data collected in Task 1, the control loop is able to send about 135 commands in 1200ms, 
                            so the time delay is dt = 1200/135 = 9ms. 
                        </p>
                        <p>Additionally, we need initial guesses for the process noise Sigma_u: </p>
                        <img class="mb-2" width="140" src="assets/img/lab7_9.png" alt="..." /><br>
                        <p> and sensor noise Sigma_z: </p>
                        <img class="mb-2" width="100" src="assets/img/lab7_10.png" alt="..." /><br>
                        <p>Plugging in dt = 9ms and dx = 2mm into the following equations: </p>
                        <img class="mb-2" width="320" src="assets/img/lab7_11.png" alt="..." /><br>
                        <p>We get sigma1^2 = sigma2^2 = 11, sigma3^2 = 50. These values are later tuned to adjust the weights of the 
                            measurements and predictions. 
                        </p>
                        <p>Putting these all together, we can then initialize the Kalman Filter: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13</div>
                        <div class="code-content"><code class="mb-0">A = np.array([[0, 1], [0, -0.00249]])   // A-matrix
B = np.array([[0], [0.00011]])          // B-matrix
dt = 9;                                 // time delay

Ad = np.eye(2) - dt * A                 // dicretized A-matrix, adjusted sign to keep model consistent 
Bd = -dt * B                            // dicretized B-matrix, adjusted sign to keep model consistent 
C=np.array([[1,0]])                     // C-matrix, adjusted sign to keep model consistent 

sig_u=np.array([[11,0],[0,11]])         // process noise 
sig_z=np.array([[50]])                  // sensor noise 

mu = np.array([[dist[0]],[0]])          // initial state
sigma = np.array([[10, 0], [0, 10]])    // uncertainty for initial state
</code></div></div>
                        
  
                     </div>

                     <div class="mb-5">
                        <div class="subheading mb-0">Task 3. Implement and Test KF in Python</div>
                        <p>The Karman Filter consists of the prediction step, where a new state is 
                            estimated with the previous state and the new control input, and the update step, where 
                            the new state estimate is corrected with the current sensor measurement:
                        </p>
                        <img class="mb-2" width="420" src="assets/img/lab7_12.png" alt="..." /><br>
                        <p> Here is the implementation of the Kalman Filter in Python: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11</div>
                        <div class="code-content"><code class="mb-0">def kf(mu,sigma,u,y):
    mu_p = Ad.dot(mu) + Bd.dot(u)                                       // prediction 
    sigma_p = Ad.dot(sigma.dot(Ad.transpose())) + sig_u                 // prediction 

    sigma_m = C.dot(sigma_p.dot(C.transpose())) + sig_z                 // update 
    kkf_gain = sigma_p.dot(C.transpose().dot(np.linalg.inv(sigma_m)))   // update 
    y_m = y-C.dot(mu_p)                                                 // update 
    mu = mu_p + kkf_gain.dot(y_m)                                       // update 
    sigma=(np.eye(2)-kkf_gain.dot(C)).dot(sigma_p)                      // update 

    return mu,sigma</code></div></div>
                        <p> To call the Kalman Filter: </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5</div>
                        <div class="code-content"><code class="mb-0">KF_distance = []

for i in range(len(time)):
    mu, sigma = kf(mu, sigma, u[i], dist[i]) // Run Kalman filter for each time step
    KF_distance.append(mu[0][0])</code></div></div>
                        <p>I tested the Kalman Filter on a set of data collected in Lab 5: PID Position Control. With small Sigma_z/Sigma_u values, 
                            we put more confidence in the current measurements and less in the predictions, 
                            and the Kalman Filter follows the ToF sensor data very closely: </p>
                        <img class="mb-2" width="420" src="assets/img/lab7_13.png" alt="..." /><br>
                        <p>As we increase the Sigma_z/Sigma_u ratio, we trust the current measurements less and the predictions more, 
                            the Kalman Filter follows a smooth path which is what we would expect for a sensor with faster sampling rate:  </p>
                        <img class="mb-2" width="420" src="assets/img/lab7_14.png" alt="..." /><br>
                        <p>But when the Sigma_z/Sigma_u ratio is too large, we are not trusting the current measurements at all but 
                            heavily rely on the prediction, which can give us predctions that are significantly off and not useable:  </p>
                        <img class="mb-2" width="420" src="assets/img/lab7_15.png" alt="..." /><br>
                        <p>From there, I found a suitable range of KF covariance matrices values for the next step. </p>
                        

                        <div class="mb-5">
                            <div class="subheading mb-0">Task 4. Implement KF on the Robot</div>
                            <p>I implementaed the Kalman Filter on the robot: </p>
                            <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32<br>33<br>34<br>35<br>36</div>
                            <div class="code-content"><code class="mb-0">#include <BasicLinearAlgebra.h>                //Use this library to work with matrices:
using namespace BLA;     //This allows me to declare a matrix

// Declare and initialize Ad, Bd, C, matrices 
Matrix<2,2> Ad = {1, -8.9,
                 0, 1.022161}; 
...

// Declare and initialize covariance matrices sig_u and sig_z
Matrix<2,2> sig_u = {1, 0,
                     0, 1}; 
...

// Declare control input u and measurement y
Matrix<1> u; 
...
    
// Declare and initialize initial states mu and initial uncertainty sigma
Matrix<2,1> mu = {dist[0], 
                  0}; 
...

// Implementation of Kalman Filter
void KF(float ui, float yi) {
  u = {ui};
  y = {yi};

  Matrix<2,1> mu_p = Ad*x + Bd*u;                   // prediction 
  Matrix<2,2> sig_p = Ad*sig*~Ad + sig_u;           // prediction 

  Matrix<1,1> sig_m = C*sig_p*~C + sig_z;           // update
  Matrix<2,1> kf_gain = sig_p*~C*(Invert(sig_m));   // update
  Matrix<1,1> y_m = y - C*mu_p;                     // update
  mu = mu_p + kf_gain*y_m;                          // update
  sigma = (I - kf_gain*C)*sig_p;                    // update
}</code></div></div>

                        <p>The data collected indicate the effectiveness of the Kalman Filter: </p>
                        <p>Here is a video showing the KF implemented on the robot: </p>
                        <iframe width="420" height="420"
                                    src="https://www.youtube.com/embed/fpn5lwr5Fog">
                        </iframe>
                        <p>Retriving the data from the robot confirms that the KF is implemented correctly:</p>
                        <img class="mb-2" width="420" src="assets/img/lab7_16.png" alt="..." /><br>
      
                        </div>
                            
                    
                     </div>

                    


            </section>
            <!-- Lab 8-->
            <section class="resume-section" id="Lab 8">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 8 <span class="text-primary">Stunt!</span></h2>
                    <p class="mb-5">The goal of this lab is to drive the robot towards the wall, do a backflip about 1ft away from the wall, drive back, and do so as fast as possible. </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Hardware</div>
                        <p>In order for the robot to perform a backflip, its center of mass must be close to the front, as shown below. 
                            Therefore I placed batteries in the front pocket of the car and added additional mass such as bolts. 
                        </p>
                        <img class="mb-2" width="240" src="assets/img/lab8_1.png" alt="..." /><br>
                        <p>Since I'm doing this stunt at home, in order to recreate the sticky pad setup in the lab environment, I covered
                             the floor around 1 ft away from the wall with tape, and then removed the tape but intentially left some sticky residuals on the floor. 
                             This will make it easier for the car to backflip.  
                        </p>
                     </div>

                     <div class="mb-5">
                        <div class="subheading mb-0">Implementation</div>
                        <p>The stunt can be broken down into 3 stages: 1) move forward, 2) flip, and 3) adjust direction & drive back. 
                            I used open loop control for all three steps. 
                            First, I accelerate the car as fast as possible towards the wall for a set amount of time, and then rapidly brake the 
                            car by reversing the wheels with the maximum PWM for a very brief second. The combine effects of displaced center of mass, 
                            sticky floor, and active braking caused the car to backflip. The following is the pseudo code of part of the program to achieve this. 
                        </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14</div>
                        <div class="code-content"><code class="mb-0">initialize motors
initialize ToF and IMU sensor 
while true{
    record time
    record ToF and IMU sensor data; 
    if (time < move forward end time)
        set forward speed to max speed
    else if (time < active brake end time)
        reverse motors with max speed
    else if (time < idle end time)
        stop motors
    else if (time < move back end time)
        set left and right wheel PWM based on IMU yaw readings to adjust direction when heading back
    else if (time > max time)
        stop motors
}</code></div></div>
                        
                        <p>Data from one of the test runs is plotted below. The 3 stages are labeled in the plot. The entire test run took about 
                            4.5 seconds (the first 0.3s of data is lost in this test run), 
                            and the maximum speed achieved can be calculated from the plot, which is about 3.6m/s. 
                            Note that when the car is moving away from the wall, the ToF sensor data is not trustworthy since the nearest obstacle
                            is outside the measuring range of the sensor. 
                        </p>
                        <img class="mb-2" width="420" src="assets/img/lab8_2.png" alt="..." /><br>
                        
                        <p>Below are videos showing repeated testruns of the stunt: </p>
                        <iframe width="420" height="360"
                                    src="https://www.youtube.com/embed/r28SlqU4JTo">
                        </iframe><br>
                        <iframe width="420" height="360"
                                    src="https://www.youtube.com/embed/jbsTltJ3OQ0">
                        </iframe><br>
                        <iframe width="420" height="360"
                                    src="https://www.youtube.com/embed/gW4Az598ueQ">
                        </iframe>
                        
      
                        </div>
                            
                    
                     </div>

                     
                     
                     


            </section>
            <!-- Lab 9-->
            <section class="resume-section" id="Lab 9">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 9 <span class="text-primary">Mapping</span></h2>
                    <p class="mb-5">The goal of this lab is to map out a static room by placing the robot at different locations in the room, 
                        rotate the robot while collecting the ToF sensor distance data, and reconstruct the map with the yaw angle and distance information. 
                        The map will be used in the next two labs for localization and navigation.  </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Orientation PID Control</div>
                        <p>
                            The quality of the map depends on whether the robot is able to accurately report its yaw angle while measuring the distance. 
                            To have a better control of the yaw, I implemented PID controller on the orientation control. The robot will rotate a set amount 
                            of degrees, stop to take measurement, and rotate again until it cover the entire 360 degrees. 
                        </p>
                        
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21</div>
                        <div class="code-content"><code class="mb-0">case MAPPING:{
    extract command variables, including Kp, Ki, Kd, increment, errBound
    initialize IMU, motor, and ToF
    measurement_taken = false
    
    while(yaw < 360){
        Record yaw from IMU
        if (measurement_taken){ 
            set_point = yaw + increment
            measurement_taken = false
        }else{ 
            if (abs(yaw - set_point) < errBound){
                stop motors
                take 5 ToF measurements and average them
                measurement_taken = true;
            }else{
                perform rotation PID to set_point
            }
    }
    send data via bluetooth
}break</code></div></div>
                    <p>
                        The Controller works as expected. The rotation is slightly off-axis, as shown in the video below, and there is 
                        a drift about 15 degress per revolution. I applied a constant multiplier to account for the drift, and the 
                        corrected data is shown below. 
                    </p>
                    <iframe width="420" height="360"
                                    src="https://www.youtube.com/embed/lrh0mN0tqAk">
                    </iframe><br>
                    <img class="mb-2" width="420" src="assets/img/lab9_0.png" alt="..." /><br>


                     </div>

                     <div class="mb-5">
                        <div class="subheading mb-0">Data Collection</div>
                        <p>I placed the robot at each of the locations [(5,3), (0,3), (-3,-2), (5,-3)], did a full revolution, 
                            and collected the yaw and average distance data. The following plot show the yaw vs. distance for each of the locaitons in polar coordinates. 
                        </p>
                        <img class="mb-2" width="210" src="assets/img/lab9_1.png" alt="..." />
                        <img class="mb-2" width="210" src="assets/img/lab9_2.png" alt="..." /><br>
                        <img class="mb-2" width="210" src="assets/img/lab9_3.png" alt="..." />
                        <img class="mb-2" width="210" src="assets/img/lab9_4.png" alt="..." /><br>
                        </div>



                        <div class="mb-5">
                            <div class="subheading mb-0">Map Reconstruction</div>
                            <p>I performed the following transformation to the ToF sensor readings, where theta is the yaw angle, 
                                x and y are the coordinateds of the robot location. The product of this transformation matrix with ToF data = [dist; 0; 1]
                                results in the global coordinates of the walls. 
                            </p>
                            <img class="mb-2" width="210" src="assets/img/lab9_5.png" alt="..." /><br>
                            <p>The following plot shows the aggregated data from four sets of ToF sensor measurements. In addition to the tranformation applied above, 
                                I had to manually adjust the angles by about 5 deg to align the readings. These misalignments are potentially due to drifts in IMU sensor 
                                readings or the initial misalignments when placing the robots. 
                            </p>
                            <img class="mb-2" width="420" src="assets/img/lab9_6.png" alt="..." /><br>
                            <img class="mb-2" width="420" src="assets/img/lab9_7.png" alt="..." /><br>

          
                            </div>
                            
                    
                     </div>

            </section>


            <!-- Lab 10-->
            <section class="resume-section" id="Lab 10">
                <div class="resume-section-content">
                    <h2 class="mb-0">Lab 10 <span class="text-primary">Localization</span></h2>
                    <p class="mb-5">The goal of this lab is to use Bayes Filter to perform robot localization in the simulated environment.  </p>
                    
                    <div class="mb-5">
                        <div class="subheading mb-0">Bayes Filter</div>
                        <p>The Bayes Filter consists of two operations for each time step: prediction and update.
                            <li>Prediction: the system predicts the current robot state 
                                from the previous state and the control inputs through the motion model. </li>
                            <li>Update: the system corrects the prediction with the sensor measurements from the current step. </li>
                        </p>
                        <img class="mb-2" width="630" src="assets/img/lab10_1.png" alt="..." />
                        <p>
                            The Python code used in the lab is structured into several modules that implement the Bayes Filter:
                            <li>Compute Control</li>
                            <li>Odometry Motion Model</li>
                            <li>Prediction Step</li>
                            <li>Sensor Model</li>
                            <li>Update Step</li>
                        </p>
                     </div>

                    
                        <div class="mb-5">
                            <div class="subheading mb-0">Compute Control</div>
                            <p>The robot movement in one step in decomposed into initial rotation (delta_rot_1), 
                                translation (delta_trans), and final rotation (delta_rot_2). 
                                Function compute_control() calculates the robot's control inputs from the previous to current step 
                                in terms of rotational angles and transitional distances from the previous and current odometry poses. 
                                Both rotation angles are normalized to the range (-180, 180) to maintain consistency.
                            </p>
                            <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>12<br>23<br>24<br>25</div>
                        <div class="code-content"><code class="mb-0">def compute_control(cur_pose, prev_pose):
    """ Given the current and previous odometry poses, this function extracts
    the control information based on the odometry motion model.

    Args:
        cur_pose  ([Pose]): Current Pose
        prev_pose ([Pose]): Previous Pose 

    Returns:
        [delta_rot_1]: Rotation 1  (degrees)
        [delta_trans]: Translation (meters)
        [delta_rot_2]: Rotation 2  (degrees)
    """
    # Intial rotation
    rot_1 = np.degrees(np.arctan2(cur_pose[1] - prev_pose[1], cur_pose[0] - prev_pose[0]) - prev_pose[2])
    delta_rot_1 = mapper.normalize_angle(rot_1)

    # Translation
    delta_trans = np.sqrt((cur_pose[1] - prev_pose[1])**2 + (cur_pose[0] - prev_pose[0])**2)

    # Final rotation
    rot_2 = cur_pose[2] - prev_pose[2] - delta_rot_1
    delta_rot_2 = mapper.normalize_angle(rot_2)

    return delta_rot_1, delta_trans, delta_rot_2</code></div></div>
                        </div>
                     

                     <div class="mb-5">
                        <div class="subheading mb-0">Odometry Motion Modell</div>
                        <p>In this step, we want to find the probability that the robot ends up at a new position x'
                            given its previous position x and the control input u. 
                            We first use the compute_control() from the previous step to extract the control inputs, 
                            and then calculate their probability using Gaussian distributions with the given standard deviation. 
                        </p>
                        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>12<br>23<br>24<br>25</div>
                    <div class="code-content"><code class="mb-0">def odom_motion_model(cur_pose, prev_pose, u):
    """ Odometry Motion Model

    Args:
        cur_pose  ([Pose]): Current Pose
        prev_pose ([Pose]): Previous Pose
        (rot1, trans, rot2) (float, float, float): A tuple with control data in the format 
                                                    format (rot1, trans, rot2) with units (degrees, meters, degrees)


    Returns:
        prob [float]: Probability p(x'|x, u)
    """
    # Extract control inputs
    delta_rot_1, delta_trans, delta_rot_2 = compute_control(cur_pose, prev_pose)

    # Probabilities of the control inputs
    prob_rot_1 = loc.gaussian(delta_rot_1, u[0], loc.odom_rot_sigma)
    prob_trans = loc.gaussian(delta_trans, u[1], loc.odom_trans_sigma)
    prob_rot_2 = loc.gaussian(delta_rot_2, u[2], loc.odom_rot_sigma)

    # Assume independency
    prob = prob_rot_1 * prob_trans * prob_rot_2

    return prob</code></div></div>

                 </div>





             <div class="mb-5">
                <div class="subheading mb-0">Prediction Step</div>
                <p>In this step, we loop through all the grids to calculate the probabliity of of the robot ending in that
                    grid cell. To speed up the computation, we ignore any cells with probablity less than 0.0001. 
                </p>
                <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>12<br>23<br>24<br>25</div>
            <div class="code-content"><code class="mb-0">def prediction_step(cur_odom, prev_odom):
""" Prediction step of the Bayes Filter.
Update the probabilities in loc.bel_bar based on loc.bel from the previous time step and the odometry motion model.
Args:
    cur_odom  ([Pose]): Current Pose
    prev_odom ([Pose]): Previous Pose
"""
# Contorl input 
u = compute_control(cur_odom, prev_odom)

# Loop through each grid cell
for x1 in range(mapper.MAX_CELLS_X):
    for y1 in range(mapper.MAX_CELLS_Y):
        for theta1 in range(mapper.MAX_CELLS_A):
            # Ignore low probability grids
            bel = loc.bel[x1, y1, theta1]
            if bel >= 0.0001:
                # Loop through all possible current states
                for x2 in range(mapper.MAX_CELLS_X):
                    for y2 in range(mapper.MAX_CELLS_Y):
                        for theta2 in range(mapper.MAX_CELLS_A):  
                            prob = odom_motion_model(mapper.from_map(x2, y2, theta2), mapper.from_map(x1, y1, theta1), u)
                            loc.bel_bar[x2, y2, theta2] += (prob * bel)
# Normalize bel bar
loc.bel_bar /= np.sum(loc.bel_bar)</code></div></div>

         </div>

         <div class="mb-5">
            <div class="subheading mb-0">Sensor Model</div>
            <p>In this step, we use an array of observations and calculate the probabilities 
                of each of them given the current state. 
            </p>
            <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17</div>
        <div class="code-content"><code class="mb-0">def sensor_model(obs):
    """ This is the equivalent of p(z|x).


    Args:
        obs ([ndarray]): A 1D array consisting of the true observations for a specific robot pose in the map 

    Returns:
        [ndarray]: Returns a 1D array of size 18 (=loc.OBS_PER_CELL) with the likelihoods of each individual sensor measurement
    """
    #Initialize array
    prob_array = np.zeros(mapper.OBS_PER_CELL)
    
    for i in range(mapper.OBS_PER_CELL):
        prob_array[i] = loc.gaussian(loc.obs_range_data[i], obs[i], loc.sensor_sigma)

    return prob_array</code></div></div>

     </div>


     <div class="mb-5">
        <div class="subheading mb-0">Update Step</div>
        <p>In this step, we take in the sensor data, combine it with previously calcuated bel_bar to update the believe. 
        </p>
        <div class="code-container"><div class="line-numbers">1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13</div>
    <div class="code-content"><code class="mb-0">def update_step():
    """ Update step of the Bayes Filter.
    Update the probabilities in loc.bel based on loc.bel_bar and the sensor model.
    """
    # For each gridcell
    for x in range(mapper.MAX_CELLS_X):
        for y in range(mapper.MAX_CELLS_Y):
            for theta in range(mapper.MAX_CELLS_A):
                p = sensor_model(mapper.get_views(x, y, theta))
                loc.bel[x, y, theta] = np.prod(p) * loc.bel_bar[x, y, theta]

    # Normalize
    loc.bel /= np.sum(loc.bel)</code></div></div>

 </div>

 <div class="mb-5">
    <div class="subheading mb-0">Results</div>
    <p>Below shows the robot running in the simulator. Even with low accuracy in the odometry data (red), the predicted trajectory 
        follows the real trajectory closly. 
    </p>
    

</div>


            </section>



    </body>
</html>
